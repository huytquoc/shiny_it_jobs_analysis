{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import sys\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = 'https://www.monster.com/jobs/browse/q-it-jobs.aspx'\n",
    "#Sample Job link\n",
    "url_job = 'https://www.monster.com/jobs/q-it-operations-manager-jobs.aspx' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the list of Job and its link from the page: url = 'https://www.monster.com/jobs/browse/q-it-jobs.aspx'\n",
    "def get_JobLink(url):\n",
    "    try:\n",
    "        page = urllib2.urlopen(url, timeout = 1)\n",
    "    except urllib2.URLError, e:\n",
    "        raise MyException(\"There was an error: %r\" % e)\n",
    "    else:\n",
    "        s = soup(page, \"html.parser\")\n",
    "        tags = s.find('ul','card-columns browse-all')\n",
    "        links = tags.find_all('a')\n",
    "    \n",
    "    Job = list()\n",
    "    Link = list()\n",
    "    for link in links:\n",
    "        if(link.text !='Select State'):\n",
    "            j = link.text\n",
    "            l = link['href']\n",
    "        #print 'Job: %s, Link: %s' %(j,l)\n",
    "            Job.append(j.replace(' Jobs',\"\"))\n",
    "            Link.append(l)\n",
    "    Job_ = pd.DataFrame({'job':Job,'link':Link})\n",
    "    return Job_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_Pages(links):\n",
    "    \n",
    "    Pages = list()\n",
    "    \n",
    "    for link in links:\n",
    "        if link != \"\":\n",
    "            try:\n",
    "                page = urllib2.urlopen(link, timeout = 3)\n",
    "            except urllib2.HTTPError:\n",
    "                Pages.append('Error: 404')\n",
    "            except urllib2.URLError, e:\n",
    "                Pages.append('Error: Timeout 3')\n",
    "            except Exception:\n",
    "                Pages.append('Error: Exception')\n",
    "            else:\n",
    "                s = soup(page, \"html.parser\")\n",
    "                Pages.append(s)\n",
    "        else:    \n",
    "            Pages.append('Error: Blank URL')\n",
    "                         \n",
    "    return Pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extracting the Number of Posting Info\n",
    "def get_NumJob(s): # input: soup\n",
    "    if isinstance(s,soup):\n",
    "        t = s.find('div', attrs={\"class\":'col-xs-12 jsresultsheader'}).h2.text\n",
    "        p = re.search(re.compile(' \\d+( |\\+)'),t)\n",
    "        num = t[p.start():p.end()-1]\n",
    "        num = int(num.strip())\n",
    "    else:\n",
    "        num = np.nan\n",
    "    return num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_Nums(pages): #pages of beautifulsoup\n",
    "    Num = list()\n",
    "    for i in range(len(pages)):\n",
    "        #print i\n",
    "        num = get_NumJob(pages[i])\n",
    "        Num.append(num)\n",
    "    return Num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extracting the City and the State information of a Post\n",
    "def get_City_State(s, job_title): # input: soup\n",
    "    City = list()\n",
    "    State = list()\n",
    "    Link = list()\n",
    "    Job = list()\n",
    "    \n",
    "    def add_item(city, state, url, job_title = job_title):\n",
    "        City.append(city)\n",
    "        State.append(state)\n",
    "        Link.append(url)\n",
    "        Job.append(job_title)\n",
    "        \n",
    "    \n",
    "    if isinstance(s,soup):\n",
    "        tags = s.find('li',attrs={\"id\":'backlinkingwidgetCity'})\n",
    "        try:\n",
    "            links = tags.find_all('a')\n",
    "        except AttributeError:\n",
    "            add_item(city = \"\", state = \"\", url = \"\")\n",
    "        else:\n",
    "            for link in links:\n",
    "                if(\"title\" in link.attrs):\n",
    "                    t = link['title']\n",
    "                    p = re.search(re.compile(',\\s*([^,]+)$'),t)\n",
    "                    state = t[p.start()+2:len(t)]\n",
    "                    city = link.text\n",
    "                    url = link['href']\n",
    "\n",
    "                    add_item(city, state, url)\n",
    "    else:\n",
    "        add_item(city = \"\", state = \"\", url = \"\")\n",
    "        \n",
    "    Location_ = pd.DataFrame({'job':Job, 'city':City,'state':State, 'link': Link})\n",
    "    return Location_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_Job_Location(pages,jobs): #input: list of Pages and the respective Job Titles\n",
    "    Job_Location = pd.DataFrame()\n",
    "    for i in range(len(pages)):\n",
    "        job_loc = get_City_State(pages[i],jobs[i])\n",
    "        Job_Location = pd.concat([Job_Location,job_loc], ignore_index=True)\n",
    "        #print i\n",
    "    return Job_Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extracting the Posting time information\n",
    "def get_Job_PostTime(s, job, state, city): # input: soup\n",
    "    Time = list()\n",
    "    Link = list()\n",
    "    Job = list()\n",
    "    Title = list()\n",
    "    State = list()\n",
    "    City = list()\n",
    "    \n",
    "    def add_item(title, time, url, job = job, state = state, city = city):\n",
    "        Title.append(title)\n",
    "        Time.append(time)\n",
    "        Link.append(url)\n",
    "        Job.append(job)\n",
    "        State.append(state)\n",
    "        City.append(city)\n",
    "        \n",
    "    \n",
    "    if isinstance(s,soup):\n",
    "        tags = s.find_all('article',attrs={\"class\":'js_result_row'})\n",
    "        for tag in tags:\n",
    "            time = tag.time['datetime']\n",
    "            url = tag.find('a')['href']\n",
    "            title = tag.find('a')['title']\n",
    " \n",
    "            add_item(title, time, url)\n",
    "    else:\n",
    "        add_item(title = \"\", time = \"\", url = \"\")\n",
    "        \n",
    "    PostTime_ = pd.DataFrame({'job':Job, 'title':Title,'time':Time, 'link': Link, 'state': State, 'city': City})\n",
    "    return PostTime_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_Job_Time(pages,jobs,states,cities): #input: list of Pages of Location and the respective Job_Location.job\n",
    "    Job_Time = pd.DataFrame()\n",
    "    for i in range(len(pages)):\n",
    "        job_time = get_Job_PostTime(pages[i],jobs.values[i],states.values[i],cities.values[i])\n",
    "        Job_Time = pd.concat([Job_Time,job_time], ignore_index=True)\n",
    "        #print i\n",
    "    return Job_Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_to_pickle(data, name):\n",
    "    if isinstance(data,list):\n",
    "        with open(name, 'wb') as f:\n",
    "            pickle.dump(data, f)\n",
    "    elif isinstance(data, pd.DataFrame) or isinstance(data, pd.Series):\n",
    "        data.to_pickle(name)\n",
    "    else:\n",
    "        return \"Not valid Data Type. Neither List, nor Pandas Object\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_from_pickle(pickle_file_name):\n",
    "    \n",
    "    #Restoring Objects from Pickle files\n",
    "    with open(pickle_file_name) as f:\n",
    "        loaded_obj = pickle.load(f)\n",
    "    \n",
    "    #Restoring DataFrame Objects from Pickle files\n",
    "    # df2 = pd.read_pickle('my_df.pickle')\n",
    "    \n",
    "    return loaded_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Page Level 1 = IT Jobs Home page with list of Job Titles\n",
    "#Page Level 2 = Job Title Home Page with list of Postings for the selected Title\n",
    "#Page Level 3 = List of Jobs on Location with list of Postings for the selected Title and Location\n",
    "\n",
    "def extract_from_P3(Jobs_Location):\n",
    "    NumL = [0]*len(Jobs_Location)\n",
    "    Job_Time = pd.DataFrame()\n",
    "    files = filter(lambda x: re.match(\"webscrapping_Pages_Location\",x), os.listdir('./'))\n",
    "    \n",
    "    for file_ in files:       \n",
    "        Pages = get_from_pickle(file_)\n",
    "        \n",
    "        n = int(re.search(re.compile('\\d+'),file_).group())-1\n",
    "        from_ = n*200\n",
    "        to_ = from_ + len(Pages)\n",
    "        Jobs = Jobs_Location.job[from_:to_]\n",
    "        States = Jobs_Location.state[from_:to_]\n",
    "        Cities = Jobs_Location.city[from_:to_]\n",
    "        \n",
    "        job_time = get_Job_Time(Pages,Jobs,States,Cities)\n",
    "        Job_Time = pd.concat([Job_Time,job_time], ignore_index=True)\n",
    "        \n",
    "        NumL[from_:to_] = get_Nums(Pages)\n",
    "        \n",
    "        print n\n",
    "        \n",
    "    return Job_Time, NumL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def download_Pages_toFile(Jobs_Location, step=200, get_all=False):\n",
    "    n = 0\n",
    "    from_ = n*step\n",
    "    to_ = from_ + step\n",
    "    \n",
    "    while from_ < len(Jobs_Location.link):\n",
    "        to_ = to_ if to_< len(Jobs_Location.link) else len(Jobs_Location.link)\n",
    "        links = Jobs_Location.link[from_:to_]\n",
    "        Pages = get_Pages(Jobs_Location.link)\n",
    "        \n",
    "        file_name = \"webscrapping_Pages_Location\" + \"_\" + str(n) + \".pickle\"\n",
    "        save_to_pickle(Pages_Location,file_name)\n",
    "        \n",
    "        if(get_all):\n",
    "            n = n+1\n",
    "            from_ = n*200\n",
    "            to_ = from_ + step\n",
    "        else:\n",
    "            from_ = len(Jobs_Location.link) + 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_Num_Location(Jobs_Location):\n",
    "    NumL = [0]*len(Jobs_Location)\n",
    "    #Job_Time = pd.DataFrame()\n",
    "    files = filter(lambda x: re.match(\"webscrapping_Pages_Location\",x), os.listdir('./'))\n",
    "    \n",
    "    for file_ in files:       \n",
    "        Pages = get_from_pickle(file_)\n",
    "        \n",
    "        n = int(re.search(re.compile('\\d+'),file_).group())-1\n",
    "        from_ = n*200\n",
    "        to_ = from_ + len(Pages)\n",
    "        #Jobs = Jobs_Location.job[from_:to_]\n",
    "        \n",
    "        #job_time = get_Job_Time(Pages,Jobs)\n",
    "        #Job_Time = pd.concat([Job_Time,job_time], ignore_index=True)\n",
    "        \n",
    "        NumL[from_:to_] = get_Nums(Pages)\n",
    "        \n",
    "        print n\n",
    "        \n",
    "    return NumL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IT_Jobs = get_JobLink(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Scrapping Pages of IT Jobs\n",
    "Pages_of_IT_Jobs = get_Pages(IT_Jobs.link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Saving the IT Jobs Pages to file\n",
    "save_to_pickle(Pages_of_IT_Jobs,\"webscrapping_Pages_of_IT_Jobs.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extracting the Num of Job Posts on each IT Job Title Page\n",
    "NumJ = get_Nums(Pages_of_IT_Jobs)\n",
    "\n",
    "# Adding this number of Job Posts on each IT Job Title to the IT_Jobs DataFrame\n",
    "IT_Jobs['count'] = NumJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Saving IT Jobs DataFrame to file\n",
    "save_to_pickle(IT_Jobs,\"webscrapping_IT_Jobs_df.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#IT_Jobs = get_from_pickle(\"webscrapping_IT_Jobs_df.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Jobs_Location = get_Job_Location(Pages_of_IT_Jobs, IT_Jobs.job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Jobs_Location = get_from_pickle(\"webscrapping_Jobs_Location_df.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "download_Pages_P3(Jobs_Location, get_all=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "1\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "2\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "data = extract_from_P3(Jobs_Location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NumL = data[1]\n",
    "Job_Time = data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Job_Location['count'] = NumL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_to_pickle(Jobs_Location, \"webscrapping_Jobs_Location_df.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save Job_Time into File\n",
    "save_to_pickle(Job_Time, \"webscrapping_Job_Time_df.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Pages_Location = get_from_pickle(\"webscrapping_Pages_of_IT_Jobs.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "882"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Pages_Location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Job_Location = get_Job_Location(Pages_Location, IT_Jobs.job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Job_Location.to_csv('webscrapping_Jobs_Location_df.csv', encoding = \"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Job_Time.to_csv('webscrapping_Job_Time_df.csv', encoding = \"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
